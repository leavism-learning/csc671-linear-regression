{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective\n",
    "The objective for this assignment is to take the in-class Multi-Layer Perceptron code we used for classification and modify it for regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approach\n",
    "## MLP Class\n",
    "We write an MLP class that inherits `torch.nn.Module`, the basic Neural Network module containing the required functions we'll use for Linear Regression.\n",
    "\n",
    "The `torch.nn.Sequential` class creates a sequential container that allows us to manually call a sequence of modules. In effect, it enables us to transform the container as needed, like creating three `torch.nn.Linear` layers. The input to the first layer should be the number of features and the output of the last layer should be 1. In this case, we'll call the Sigmoid activation function to see a non-linear fit to the data. This will be graphed later in the report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class MLP(torch.nn.Module):\n",
    "\tdef __init__(self, num_features):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.all_layers = torch.nn.Sequential(\n",
    "            # 1st hidden layer\n",
    "            torch.nn.Linear(num_features, 5),\n",
    "            torch.nn.Sigmoid(),\n",
    "            # 2nd hidden layer\n",
    "            torch.nn.Linear(5, 2),\n",
    "            torch.nn.Sigmoid(),\t\t\t\t\t\t  \n",
    "            # output layer\n",
    "            torch.nn.Linear(2, 1),\n",
    "        )\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\tlogits = self.all_layers(x)\n",
    "\t\treturn logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset, Data normalization, and Dataloader\n",
    "\n",
    "Given the data below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.tensor([245.0, 273.0, 304.0, 331.0, 347.0, 360.0, 387.0, 438.0, 493.0, 547.0]).view(-1,1)\n",
    "y_train = torch.tensor([232.3, 241.1, 257.4, 301.5, 324.6, 350.2, 362.3, 389.0, 398.2, 401.8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can create a `MyDataset` class modeled after the one we discussed in the lecture. This time, instead of inheriting from the `Dataset` class, we'll inherit from the `TensorDataset` class as our data is already tensorized.\n",
    "\n",
    "The `MyDataset` class is a map-style dataset and needs to implement the `__getItem__()` and `__len__()` protocols. By defining these methods, we enable the use of the `DataLoader` utility class, allowing us to easily iterate through the dataset during our training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset \n",
    "\n",
    "class MyDataset(TensorDataset):\n",
    "\tdef __init__(self, X, y):\n",
    "\t\tself.features = X\n",
    "\t\tself.labels = y\n",
    "\n",
    "\tdef __getitem__(self, index):\n",
    "\t\tx = self.features[index]\n",
    "\t\ty = self.labels[index]\n",
    "\t\treturn x, y\n",
    "\t\n",
    "\tdef __len__(self):\n",
    "\t\treturn self.labels.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classification example during lecture did not use normalized data because it was already centered at zero, and had relatively small values.\n",
    "\n",
    "In our case, we will normalize the data by doing z-score standardization using the mean and standard deviation of the given data. We'll also later use these values to make predictions when plotting the regression curve.\n",
    "\n",
    "We then put our normalized data into the `MyDataset` class, which is then inserted into the `DataLoader` utility class for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader \n",
    "\n",
    "X_mean, X_std = X_train.mean(), X_train.std()\n",
    "y_mean, y_std = y_train.mean(), y_train.std()\n",
    "\n",
    "X_normalized = (X_train - X_mean) / X_std\n",
    "y_normalized = (y_train - y_mean) / y_std\n",
    "\n",
    "train_ds = MyDataset(X_normalized, y_normalized)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "\tdataset=train_ds,\n",
    "\tbatch_size=5,\n",
    "\tshuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop\n",
    "\n",
    "The code for the training is very similar to the classification example from lecture, so I'd like to analyze the code in this report.\n",
    "Before the training loop, we do a bit of setup. We first set a seed by calling `torch.manual_seed()` to ensure reproducibility of results in this report.\n",
    "We then initialize our MLP model and optimizer. The MLP model takes in the number of features (which is 1 in our case). Our optimizer will use the Stochastic Gradient Descent algorithm with parameters from our MLP model and a learning rate of 0.5.\n",
    "\n",
    "### Stochastic Gradient Descent (SGD)\n",
    "SGD is an optimization algorithm used to minimize the loss function in deep learning models. Stochastic refers to how the gradient is computed and the weights are updated for each training batch, as opposed to the entire training dataset.\n",
    "\n",
    "### Learning Rate\n",
    "The learning rate is a hyperparameter of the optimization algorithm that determines the step size of each iteration when moving towards a minimum of the loss function. It controls how much we adjust the model in response to the estimated error each time the model weights are updated.\n",
    "\n",
    "A high learning rate result in the model converging faster, but can overshoot the optimal point. This could even lead to higher loss or the model may fail to converge entirely.\n",
    "\n",
    "A low learning rate will converge slowly, which may allow the model to reach a more precise loss minimum at the cost of time. There's a chance that the model can get stuck in an undesired local minimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "model = MLP(num_features=1)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now comes the actual training loop. We declare the number of epochs and a list to store the loss value at the end of each epoch. These loss values are used later to visually see our loss regression on a graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 200 \n",
    "losses = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then create our training loop to run `num_epochs` times. At the beginning of each epoch iteration, we set our `model` to training mode, as opposed to evaluation mode with `model.train()`.\n",
    "\n",
    "With each epoch iteration, we create a batch loop which iterates through our dataset.\n",
    "\n",
    "Each batch iteration will retrieve the features and labels, passes the features into our model to get predictions, and calculates the loss for that batch by using `torch.nn.functional.mse_loss()`. We then call `zero_grad()` on our optimizer to set the gradients of all model parameters to zero as gradients accumlate by default in PyTorch. We call `backward()` to compute the gradient of the loss with respect to the model and then updates the model parameters with the newly calculated gradient by calling `step()` on the optimizer.\n",
    "\n",
    "At the end of each batch, we add our loss to the `losses` list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model = model.train()\n",
    "    for batch_idx, (features, labels) in enumerate(train_loader):\n",
    "        preds = model(features)\n",
    "        loss = F.mse_loss(preds.squeeze(), labels.float())\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    losses.append(loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion - Evaluating the Model\n",
    "\n",
    "We can now plot the regression curve! We start off by generating the points as x values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_range = torch.arange(200, 600, 0.1).view(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then normalize these points from the `x_mean` and `x_std` from earlier, and use normalized points to make predictions from our now trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_range_normalized = (X_range - X_mean) / X_std\n",
    "\n",
    "with torch.no_grad():\n",
    "    y_range_preds = model(X_range_normalized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we un-normalize the prediction values using the original `y_mean` and `y_std`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_range_unnormalized = (y_range_preds.squeeze() * y_std) + y_mean\n",
    "y_range_unnormalized = y_range_unnormalized.numpy()\n",
    "X_range = X_range.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This provides us with everything we need to plot the regression curve! Notice that the curve is non-linear because we used the Sigmoid activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X_train, y_train, color='blue', label='Training data', s=100)\n",
    "plt.plot(X_range, y_range_unnormalized, color='red', label='Regression curve')\n",
    "plt.xlabel('X values')\n",
    "plt.ylabel('Predictions')\n",
    "plt.title('MLP Regression Curve with Non-Linear Activation')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
