{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective\n",
    "The objective for this assignment is to take the in-class Multi-Layer Perceptron code we used for classification and modify it for regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approach\n",
    "## MLP Class\n",
    "We write an MLP class that inherits `torch.nn.Module`, the basic Neural Network module containing the required functions we'll use for Linear Regression.\n",
    "\n",
    "The `torch.nn.Sequential` class creates a sequential container that allows us to manually call a sequence of modules. In effect, it enables us to transform the container as needed, like creating three `torch.nn.Linear` layers. The input to the first layer should be the number of features and the output of the last layer should be 1. In this case, we'll call the Sigmoid activation function to see a non-linear fit to the data. This will be graphed later in the report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class MLP(torch.nn.Module):\n",
    "\tdef __init__(self, num_features):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.all_layers = torch.nn.Sequential(\n",
    "            # 1st hidden layer\n",
    "            torch.nn.Linear(num_features, 5),\n",
    "            torch.nn.Sigmoid(),\n",
    "            # 2nd hidden layer\n",
    "            torch.nn.Linear(5, 2),\n",
    "            torch.nn.Sigmoid(),\t\t\t\t\t\t  \n",
    "            # output layer\n",
    "            torch.nn.Linear(2, 1),\n",
    "        )\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\tlogits = self.all_layers(x)\n",
    "\t\treturn logits"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
